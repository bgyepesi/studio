{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plot Images\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "plt.style.use('ggplot')\n",
    "import matplotlib\n",
    "# Set GPU usage\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Plotly \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "from studio.evaluation.keras import metrics, utils, visualizer\n",
    "from studio.evaluation.keras.evaluators import CNNEvaluator\n",
    "from studio.evaluation.keras.evaluators import Evaluator\n",
    "from studio.evaluation.keras.evaluators import SequentialCNNEvaluator\n",
    "from studio.evaluation.keras.evaluators import VisualQAEvaluator\n",
    "from visual_qa.visual_qa import VisualQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example paths\n",
    "report_dir = '../report/'\n",
    "model_path = '/data/models/example_model/model_max_acc_1gpu.h5'\n",
    "visual_dictionary = 'qa-data/data/visual_classifier/visual_dictionary.json'\n",
    "by_definition_csv = 'qa-data/data/by-definition/by_definition_matrix.csv'\n",
    "qa_data_json = 'qa-data/data/testset.json'\n",
    "valid_evidence = 'qa-data/data/valid_evidence.json'\n",
    "visual_data_dir = '/data/datasets/visual_qa/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Object\n",
    "cnn_evaluator = CNNEvaluator(\n",
    "        concept_dictionary_path=None,\n",
    "        custom_objects=None,\n",
    "        concepts=None,\n",
    "        model_path=model_path,\n",
    "        batch_size=32,\n",
    "        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual QA Object\n",
    "visual_qa_evaluator = VisualQAEvaluator(\n",
    "    report_dir=report_dir,\n",
    "    qa_data_json = qa_data_json,\n",
    "    visual_dictionary = visual_dictionary,\n",
    "    by_definition_csv = by_definition_csv,\n",
    "    valid_evidence = valid_evidence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_testset = visual_qa_evaluator.filtered_qa_data\n",
    "filtered_diagnosis_ids = list(visual_qa_evaluator.visual_qa.visual_diagnosis_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Visual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "image_paths = []\n",
    "for sample in filtered_testset:\n",
    "    image_paths.append(os.path.join(visual_data_dir, sample[\"image_id\"]))\n",
    "    labels.append(filtered_diagnosis_ids.index(sample['diagnosis_id']))\n",
    "    \n",
    "probs = cnn_evaluator.predict(image_paths)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_result = cnn_evaluator.get_metrics(probs, labels, top_k=5, concept_labels=filtered_diagnosis_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate By-definition filter and get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_qa_result = visual_qa_evaluator.evaluate(probs, differential=True, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_qa_evaluator.plot_top_k_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.compare_visual_by_definition_results(visual_result['average']['accuracy'] ,visual_qa_result['average']['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_qa_evaluator.find_top_k_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
